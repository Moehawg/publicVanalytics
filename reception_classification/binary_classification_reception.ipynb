{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBgzHxADt6KS"
   },
   "source": [
    "# Volleyball Reception Classification Report\n",
    "\n",
    "This notebook documents the complete development process of a binary classifier to distinguish between **\"upper\"** and **\"lower\"** volleyball receptions using skeleton keypoints extracted via a YOLO pose estimation model. The workflow covers data curation, cleaning, preprocessing, model training, evaluation, data augmentation, and final conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pEktKRDt6KV"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In recent years, **sports analytics** has grown into a powerful tool for performance optimization, tactical decision-making, and even injury prevention. Within this domain, **computer vision and machine learning** are playing increasingly important roles — offering insights that were previously unachievable at scale or in real time.\n",
    "\n",
    "This project is a **case study** in building a lightweight yet effective model for classifying **volleyball reception types** (\"upper\" vs. \"lower\") using only **skeleton keypoint data**, rather than full images or video. The use of **YOLO11x pose estimation** enables us to extract 2D joint coordinates for each player from video frames — offering a **computationally efficient** alternative to using full-resolution images.\n",
    "\n",
    "The classification task revolves around analyzing the **relative body posture** of the player performing the reception:\n",
    "- In an **\"upper\" reception**, arms are typically raised above or level with the chest.\n",
    "- In a **\"lower\" reception**, arms are extended downward or outward near the legs.\n",
    "\n",
    "By analyzing only this **structured pose data**, I demonstrate that it is possible to build a binary classifier using a simple **Multilayer Perceptron (MLP)** architecture.\n",
    "\n",
    "**Project Objectives:**\n",
    "- Curate and clean a dataset of YOLO-based skeleton keypoints for volleyball receptions.\n",
    "- Build and evaluate a classifier to distinguish between \"upper\" and \"lower\" receptions.\n",
    "- Augment the dataset in a geometrically valid way to improve model generalization.\n",
    "- Document the end-to-end pipeline to demonstrate  a reproducible ML workflow.\n",
    "\n",
    "**Upper Reception:**\n",
    "\n",
    "![Upper Reception](resource/upper_22.png)\n",
    "\n",
    "**Lower Reception:**\n",
    "\n",
    "![Lower Reception](resource/lower_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8NOyYuUt6KW"
   },
   "source": [
    "## 2. Data Curation and Cleaning\n",
    "\n",
    "### Data Source and Format\n",
    "\n",
    "- The dataset is provided as a CSV file where each row is formatted as follows:\n",
    "  ```\n",
    "  kp1_x, kp1_y, kp2_x, kp2_y, ..., kpN_x, kpN_y, label\n",
    "  ```\n",
    "  where `label` is either \"upper\" or \"lower\".\n",
    "\n",
    "### Cleaning Process\n",
    "\n",
    "- Keypoint predictions generated by the pose estimation model were manually reviewed using overlay images.\n",
    "- Samples with clearly incorrect skeletons or where keypoints were extracted from the wrong person were removed.\n",
    "- Utility scripts were used to organize and curate the images and CSV files.\n",
    "\n",
    "**Sample of CSV Keypoint dataset:** ![Keypoints sample](resource/csvExample.png)\n",
    "\n",
    "**Wrong predictions of YOLO Keypoint Model:**\n",
    "\n",
    "![YOLO keypoint sample](resource/upper_25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Pne7wSat6KW"
   },
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "The CSV file is loaded, and features (keypoints) and labels are separated. The labels are mapped to binary values (\"upper\" → 1, \"lower\" → 0), and the features are scaled using StandardScaler. This helps reduce the impact of absolute positions, allowing the model to focus on relative body posture.\n",
    "\n",
    "Below is the code used for data loading and preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IHmVabuXt6KW"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_and_preprocess_data(csv_path):\n",
    "    \"\"\"\n",
    "    Load dataset from a CSV file and preprocess the data.\n",
    "    \n",
    "    Expected CSV format per row:\n",
    "        kp1_x, kp1_y, kp2_x, kp2_y, ..., kpN_x, kpN_y, label\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    X = df.iloc[:, :-1].values.astype(np.float32)\n",
    "    y = df.iloc[:, -1].values\n",
    "    label_map = {\"upper\": 1, \"lower\": 0}\n",
    "    y = np.array([label_map[label.lower()] for label in y])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# Example usage:\n",
    "csv_path = \"data/original_dataset.csv\"  # Update this path as needed\n",
    "X_scaled, y, scaler = load_and_preprocess_data(csv_path)\n",
    "print(\"Data shape:\", X_scaled.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_e-iq2hYt6KX"
   },
   "source": [
    "## 4. Model Building and Training\n",
    "\n",
    "We use a simple Multilayer Perceptron (MLP) with two hidden layers and dropout regularization. The decision to use an MLP was made because the input data (flattened keypoints) is low-dimensional and structured. If raw images were used (high-dimensional data), a CNN architecture would be more appropriate. The network architecture is as follows (Dropout is added to reduce the risk of overfitting):\n",
    "\n",
    "- **Dense(64, ReLU)** → **Dropout(0.3)**\n",
    "- **Dense(32, ReLU)** → **Dropout(0.3)**\n",
    "- **Dense(1, Sigmoid)**\n",
    "\n",
    "The dataset is split into training and test sets (80/20 split), with a validation split taken from the training set. EarlyStopping and ModelCheckpoint callbacks are used to prevent overfitting.\n",
    "\n",
    "Below is the code for model building and training:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8U3h5yakt6KY"
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def build_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "print(\"Training set shape:\", X_train.shape, \"Test set shape:\", X_test.shape)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "model = build_model(input_dim)\n",
    "model.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(\"models/volleyball_receive_classifier.h5\", monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=24,\n",
    "                    validation_split=0.1, shuffle=True, callbacks=[early_stop, checkpoint])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b7W_DHu9t6KY"
   },
   "source": [
    "# Plot training and validation curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs. Validation Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs. Validation Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPUoBtVVt6KY"
   },
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "I evaluate the model on the previously held-out test set. Below, we compute overall loss and accuracy, and also generate a detailed classification report and a confusion matrix.\n",
    "\n",
    "### Evaluation Visualizations\n",
    "\n",
    "Below are the key evaluation graphs:\n",
    "\n",
    "**Training and Validation Curves:**\n",
    "\n",
    "![Training & Validation Graphs](resource/TrainingValidationGraphs.png)\n",
    "\n",
    "**Confusion Matrix:**\n",
    "\n",
    "![Confusion Matrix](resource/ConfusionMatrix.png)\n",
    "\n",
    "**Classification Report:**\n",
    "\n",
    "![Classification Report](resource/classificationReport.png)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lkDOcIrSt6KY"
   },
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Detailed classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"lower\", \"upper\"]))\n",
    "\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"lower\", \"upper\"], yticklabels=[\"lower\", \"upper\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6mTJ7L-t6KZ"
   },
   "source": [
    "## 6. Data Augmentation\n",
    "\n",
    "Due to the small size of our initial dataset, we apply data augmentation to quadruple its size. The following augmentations are used:\n",
    "\n",
    "- **Global Noise:** Adds small Gaussian noise to all keypoints.\n",
    "- **Arm Shift:** Shifts keypoints corresponding to the arms horizontally.\n",
    "- **Global Scaling:** Scales the entire skeleton about its centroid.\n",
    "\n",
    "Below is the code for the augmentation functions and an example of how to augment the dataset."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bo6ez2jCt6KZ"
   },
   "source": [
    "def augment_global_noise(keypoints, noise_std=2.0):\n",
    "    noise = np.random.normal(0, noise_std, keypoints.shape)\n",
    "    return keypoints + noise\n",
    "\n",
    "def augment_arm_shift(keypoints, shift_range=5.0):\n",
    "    kp_aug = keypoints.copy()\n",
    "    left_arm_idx = [5, 7, 9]\n",
    "    right_arm_idx = [6, 8, 10]\n",
    "    left_shift = np.random.uniform(-shift_range, shift_range)\n",
    "    right_shift = np.random.uniform(-shift_range, shift_range)\n",
    "    for idx in left_arm_idx:\n",
    "        if idx < kp_aug.shape[0]:\n",
    "            kp_aug[idx, 0] += left_shift\n",
    "    for idx in right_arm_idx:\n",
    "        if idx < kp_aug.shape[0]:\n",
    "            kp_aug[idx, 0] += right_shift\n",
    "    return kp_aug\n",
    "\n",
    "def augment_global_scaling(keypoints, scale_range=(0.95, 1.05)):\n",
    "    kp_aug = keypoints.copy()\n",
    "    centroid = np.mean(kp_aug, axis=0)\n",
    "    scale_factor = np.random.uniform(scale_range[0], scale_range[1])\n",
    "    kp_aug = (kp_aug - centroid) * scale_factor + centroid\n",
    "    return kp_aug\n",
    "\n",
    "def augment_sample(flat_sample):\n",
    "    keypoints = flat_sample.reshape(-1, 2)\n",
    "    augmented_samples = []\n",
    "    augmented_samples.append(augment_global_noise(keypoints).flatten())\n",
    "    augmented_samples.append(augment_arm_shift(keypoints).flatten())\n",
    "    augmented_samples.append(augment_global_scaling(keypoints).flatten())\n",
    "    return augmented_samples\n",
    "\n",
    "def augment_dataset(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    feature_cols = df.columns[:-1]\n",
    "    label_col = df.columns[-1]\n",
    "    augmented_rows = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        flat_sample = row[feature_cols].values.astype(np.float32)\n",
    "        label = row[label_col]\n",
    "        augmented_rows.append(np.concatenate([flat_sample, [label]]))\n",
    "        for aug_sample in augment_sample(flat_sample):\n",
    "            augmented_rows.append(np.concatenate([aug_sample, [label]]))\n",
    "    \n",
    "    augmented_rows = np.array(augmented_rows)\n",
    "    num_keypoints = augmented_rows.shape[1] - 1\n",
    "    num_points = num_keypoints // 2\n",
    "    columns = []\n",
    "    for i in range(num_points):\n",
    "        columns.append(f\"kp{i+1}_x\")\n",
    "        columns.append(f\"kp{i+1}_y\")\n",
    "    columns.append(\"label\")\n",
    "    \n",
    "    df_aug = pd.DataFrame(augmented_rows, columns=columns)\n",
    "    df_aug.to_csv(output_csv, index=False)\n",
    "    print(f\"Augmented dataset saved to {output_csv}\")\n",
    "\n",
    "# Example usage:\n",
    "# augment_dataset(\"data/original_dataset.csv\", \"data/augmented_dataset.csv\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZalxG0bt6KZ"
   },
   "source": [
    "## 7. Inference\n",
    "\n",
    "A separate inference pipeline (provided in `inference_receive_classifier.py`) is used to predict on new keypoint data. This notebook focuses on the training, evaluation, and augmentation process. Refer to the inference script for details on how new data is processed and classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJq1Gvaot6KZ"
   },
  "source": [
   "## 8. Conclusions and Recommendations\n",
   "\n",
   "### Model Performance\n",
   "\n",
   "- The model achieved high overall accuracy with strong precision and recall on both classes, although there is concern that it might still have overfitted to the limited training data. Further evaluation on a larger or more diverse external dataset is recommended.\n",
   "- Training and validation curves indicate that early stopping and dropout helped manage overfitting. However, given the small dataset size, additional data would likely improve generalization. We tried to mitigate the limited quantity by including a wide variety of poses for both reception types and by applying diverse augmentation techniques.\n",
   "\n",
   "### Data Augmentation\n",
   "\n",
   "- Augmenting the data (via noise, arm shift, and scaling) effectively quadrupled the dataset, enhancing model robustness despite the limited original data.\n",
   "\n",
   "### Future Work\n",
   "\n",
   "- Validate the model on an external evaluation dataset to confirm generalization.\n",
   "\n",
   "Additional Considerations:\n",
   "- This model is presented as a case study to show that it is relatively simple to perform classification using keypoint data. In practice, one could extend the approach to further distinguish between different types of lower receptions (e.g., whether the ball is received perfectly in front of the body or shifted to the left or right), which could generate further insights.\n",
   "- The model is highly reliant on the quality of the keypoint data. Our results were affected by the fact that some original images were blurry or not in full HD resolution, which in turn impacted the accuracy of the keypoint predictions.\n",
   "- While YOLO was used for pose estimation in this project, alternative models such as ViTPose or other state-of-the-art methods might offer improved performance. Moreover, incorporating temporal tracking of keypoints over multiple frames (with interpolation to handle mispredictions) could further enhance real-world applicability.\n",
   "\n",
   "Overall, this project demonstrates that a relatively simple MLP can effectively classify volleyball receptions using keypoint data, but further validation and exploration of alternative pose estimation techniques are recommended to improve robustness and generalization."
  ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
